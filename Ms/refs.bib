
@article{dehejia_causal_1999,
	title = {Causal {Effects} in {Nonexperimental} {Studies}: {Reevaluating} the {Evaluation} of {Training} {Programs}},
	volume = {94},
	issn = {0162-1459},
	shorttitle = {Causal {Effects} in {Nonexperimental} {Studies}},
	url = {http://www.jstor.org/stable/2669919},
	doi = {10.2307/2669919},
	abstract = {This article uses propensity score methods to estimate the treatment impact of the National Supported Work (NSW) Demonstration, a labor training program, on postintervention earnings. We use data from Lalonde's evaluation of nonexperimental methods that combine the treated units from a randomized evaluation of the NSW with nonexperimental comparison units drawn from survey datasets. We apply propensity score methods to this composite dataset and demonstrate that, relative to the estimators that Lalonde evaluates, propensity score estimates of the treatment impact are much closer to the experimental benchmark estimate. Propensity score methods assume that the variables associated with assignment to treatment are observed (referred to as ignorable treatment assignment, or selection on observables). Even under this assumption, it is difficult to control for differences between the treatment and comparison groups when they are dissimilar and when there are many preintervention variables. The estimated propensity score (the probability of assignment to treatment, conditional on preintervention variables) summarizes the preintervention variables. This offers a diagnostic on the comparability of the treatment and comparison groups, because one has only to compare the estimated propensity score across the two groups. We discuss several methods (such as stratification and matching) that use the propensity score to estimate the treatment impact. When the range of estimated propensity scores of the treatment and comparison groups overlap, these methods can estimate the treatment impact for the treatment group. A sensitivity analysis shows that our estimates are not sensitive to the specification of the estimated propensity score, but are sensitive to the assumption of selection on observables. We conclude that when the treatment and comparison groups overlap, and when the variables determining assignment to treatment are observed, these methods provide a means to estimate the treatment impact. Even though propensity score methods are not always applicable, they offer a diagnostic on the quality of nonexperimental comparison groups in terms of observable preintervention variables.},
	number = {448},
	urldate = {2015-11-16},
	journal = {Journal of the American Statistical Association},
	author = {Dehejia, Rajeev H. and Wahba, Sadek},
	year = {1999},
	keywords = {Causality},
	pages = {1053--1062},
	file = {JSTOR Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/PB54S9BZ/Dehejia and Wahba - 1999 - Causal Effects in Nonexperimental Studies Reevalu.pdf:application/pdf}
}

@article{diamond_genetic_2012,
	title = {Genetic {Matching} for {Estimating} {Causal} {Effects}: {A} {General} {Multivariate} {Matching} {Method} for {Achieving} {Balance} in {Observational} {Studies}},
	volume = {95},
	issn = {0034-6535},
	shorttitle = {Genetic {Matching} for {Estimating} {Causal} {Effects}},
	url = {http://dx.doi.org/10.1162/REST_a_00318},
	doi = {10.1162/REST_a_00318},
	abstract = {This paper presents genetic matching, a method of multivariate matching that uses an evolutionary search algorithm to determine the weight each covariate is given. Both propensity score matching and matching based on Mahalanobis distance are limiting cases of this method. The algorithm makes transparent certain issues that all matching methods must confront. We present simulation studies that show that the algorithm improves covariate balance and that it may reduce bias if the selection on observables assumption holds. We then present a reanalysis of a number of data sets in the LaLonde (1986) controversy.},
	number = {3},
	urldate = {2015-11-16},
	journal = {Review of Economics and Statistics},
	author = {Diamond, Alexis and Sekhon, Jasjeet S.},
	month = oct,
	year = {2012},
	pages = {932--945},
	file = {Review of Economics and Statistics Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/E9QWHF2I/Diamond and Sekhon - 2012 - Genetic Matching for Estimating Causal Effects A .pdf:application/pdf;Review of Economics and Statistics Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/UF4RV9IA/REST_a_00318.html:text/html}
}

@article{imai_get-out--vote_2005,
	title = {Do {Get}-{Out}-the-{Vote} {Calls} {Reduce} {Turnout}? {The} {Importance} of {Statistical} {Methods} for {Field} {Experiments}},
	volume = {null},
	issn = {1537-5943},
	shorttitle = {Do {Get}-{Out}-the-{Vote} {Calls} {Reduce} {Turnout}?},
	url = {http://journals.cambridge.org/article_S0003055405051658},
	doi = {10.1017/S0003055405051658},
	abstract = {In their landmark study of a field experiment, Gerber and Green (2000) found that get-out-the-vote calls reduce turnout by five percentage points. In this article, I introduce statistical methods that can uncover discrepancies between experimental design and actual implementation. The application of this methodology shows that Gerber and Green's negative finding is caused by inadvertent deviations from their stated experimental protocol. The initial discovery led to revisions of the original data by the authors and retraction of the numerical results in their article. Analysis of their revised data, however, reveals new systematic patterns of implementation errors. Indeed, treatment assignments of the revised data appear to be even less randomized than before their corrections. To adjust for these problems, I employ a more appropriate statistical method and demonstrate that telephone canvassing increases turnout by five percentage points. This article demonstrates how statistical methods can find and correct complications of field experiments.},
	number = {02},
	urldate = {2015-11-16},
	journal = {American Political Science Review},
	author = {Imai, Kosuke},
	month = may,
	year = {2005},
	pages = {283--300},
	file = {Cambridge Journals Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/NJ64NAXP/displayAbstract.html:text/html;Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/F5IBDCCZ/Imai - 2005 - Do Get-Out-the-Vote Calls Reduce Turnout The Impo.pdf:application/pdf}
}

@article{freedman_weighting_2008,
	title = {Weighting regressions by propensity scores},
	volume = {32},
	issn = {0193-841X},
	doi = {10.1177/0193841X08317586},
	abstract = {Regressions can be weighted by propensity scores in order to reduce bias. However, weighting is likely to increase random error in the estimates, and to bias the estimated standard errors downward, even when selection mechanisms are well understood. Moreover, in some cases, weighting will increase the bias in estimated causal parameters. If investigators have a good causal model, it seems better just to fit the model without weights. If the causal model is improperly specified, there can be significant problems in retrieving the situation by weighting, although weighting may help under some circumstances.},
	language = {eng},
	number = {4},
	journal = {Evaluation Review},
	author = {Freedman, David A. and Berk, Richard A.},
	month = aug,
	year = {2008},
	pmid = {18591709},
	keywords = {Causality, Models, Statistical, Observation, Regression Analysis, Research Design, Selection Bias},
	pages = {392--409}
}

@article{hainmueller_entropy_2011,
	title = {Entropy {Balancing} for {Causal} {Effects}: {A} {Multivariate} {Reweighting} {Method} to {Produce} {Balanced} {Samples} in {Observational} {Studies}},
	issn = {1047-1987, 1476-4989},
	shorttitle = {Entropy {Balancing} for {Causal} {Effects}},
	url = {http://pan.oxfordjournals.org/content/early/2011/10/15/pan.mpr025},
	doi = {10.1093/pan/mpr025},
	abstract = {This paper proposes entropy balancing, a data preprocessing method to achieve covariate balance in observational studies with binary treatments. Entropy balancing relies on a maximum entropy reweighting scheme that calibrates unit weights so that the reweighted treatment and control group satisfy a potentially large set of prespecified balance conditions that incorporate information about known sample moments. Entropy balancing thereby exactly adjusts inequalities in representation with respect to the first, second, and possibly higher moments of the covariate distributions. These balance improvements can reduce model dependence for the subsequent estimation of treatment effects. The method assures that balance improves on all covariate moments included in the reweighting. It also obviates the need for continual balance checking and iterative searching over propensity score models that may stochastically balance the covariate moments. We demonstrate the use of entropy balancing with Monte Carlo simulations and empirical applications.},
	language = {en},
	urldate = {2015-11-16},
	journal = {Political Analysis},
	author = {Hainmueller, Jens},
	month = oct,
	year = {2011},
	pages = {mpr025},
	file = {Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/PGZWQUQ7/Hainmueller - 2011 - Entropy Balancing for Causal Effects A Multivaria.pdf:application/pdf;Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/2ZXUX4AB/pan.html:text/html}
}

@article{holland_statistics_1986,
	title = {Statistics and {Causal} {Inference}},
	volume = {81},
	issn = {0162-1459},
	url = {http://www.jstor.org/stable/2289064},
	doi = {10.2307/2289064},
	abstract = {Problems involving causal inference have dogged at the heels of statistics since its earliest days. Correlation does not imply causation, and yet causal conclusions drawn from a carefully designed experiment are often valid. What can a statistical model say about causation? This question is addressed by using a particular model for causal inference (Holland and Rubin 1983; Rubin 1974) to critique the discussions of other writers on causation and causal inference. These include selected philosophers, medical researchers, statisticians, econometricians, and proponents of causal modeling.},
	number = {396},
	urldate = {2015-11-16},
	journal = {Journal of the American Statistical Association},
	author = {Holland, Paul W.},
	year = {1986},
	pages = {945--960},
	file = {JSTOR Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/4KX2FT6J/Holland - 1986 - Statistics and Causal Inference.pdf:application/pdf}
}

@article{rosenbaum_central_1983,
	title = {The central role of the propensity score in observational studies for causal effects},
	volume = {70},
	issn = {0006-3444, 1464-3510},
	url = {http://biomet.oxfordjournals.org/content/70/1/41},
	doi = {10.1093/biomet/70.1.41},
	abstract = {The propensity score is the conditional probability of assignment to a particular treatment given a vector of observed covariates. Both large and small sample theory show that adjustment for the scalar propensity score is sufficient to remove bias due to all observed covariates. Applications include: (i) matched sampling on the univariate propensity score, which is a generalization of discriminant matching, (ii) multivariate adjustment by subclassification on the propensity score where the same subclasses are used to estimate treatment effects for all outcome variables and in all subpopulations, and (iii) visual representation of multivariate covariance adjustment by a two- dimensional plot.},
	language = {en},
	number = {1},
	urldate = {2015-11-16},
	journal = {Biometrika},
	author = {Rosenbaum, Paul R. and Rubin, Donald B.},
	month = apr,
	year = {1983},
	keywords = {Covariance adjustment, Direct adjustment, Discriminant matching, Matched sampling, Nonrandomized study, Standardization, Stratification, Subclassification},
	pages = {41--55},
	file = {Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/VG2W3TCJ/Rosenbaum and Rubin - 1983 - The central role of the propensity score in observ.pdf:application/pdf;Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/XQ8JVE34/41.html:text/html}
}

@article{imbens_robust_2005,
	title = {Robust, accurate confidence intervals with a weak instrument: quarter of birth and education},
	volume = {168},
	issn = {1467-985X},
	shorttitle = {Robust, accurate confidence intervals with a weak instrument},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-985X.2004.00339.x/abstract},
	doi = {10.1111/j.1467-985X.2004.00339.x},
	abstract = {Summary.  An instrument or instrumental variable manipulates a treatment and affects the outcome only indirectly through its manipulation of the treatment. For instance, encouragement to exercise might increase cardiovascular fitness, but only indirectly to the extent that it increases exercise. If instrument levels are randomly assigned to individuals, then the instrument may permit consistent estimation of the effects caused by the treatment, even though the treatment assignment itself is far from random. For instance, one can conduct a randomized experiment assigning some subjects to ‘encouragement to exercise’ and others to ‘no encouragement’ but, for reasons of habit or taste, some subjects will not exercise when encouraged and others will exercise without encouragement; none-the-less, such an instrument aids in estimating the effect of exercise. Instruments that are weak, i.e. instruments that have only a slight effect on the treatment, present inferential problems. We evaluate a recent proposal for permutation inference with an instrumental variable in four ways: using Angrist and Krueger's data on the effects of education on earnings using quarter of birth as an instrument, following Bound, Jaeger and Baker in using simulated independent observations in place of the instrument in Angrist and Krueger's data, using entirely simulated data in which correct answers are known and finally using statistical theory to show that only permutation inferences maintain correct coverage rates. The permutation inferences perform well in both easy and hard cases, with weak instruments, as well as with long-tailed responses.},
	language = {en},
	number = {1},
	urldate = {2016-02-19},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	author = {Imbens, Guido W. and Rosenbaum, Paul R.},
	month = jan,
	year = {2005},
	keywords = {Hodges–Lehmann estimate, Instrumental variable, Observational study, Permutation test, Randomization test},
	pages = {109--126},
	file = {Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/DQ43XUH6/Imbens and Rosenbaum - 2005 - Robust, accurate confidence intervals with a weak .pdf:application/pdf;Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/TPPWKSK4/abstract.html:text/html}
}

@article{cochran_effectiveness_1968,
	title = {The {Effectiveness} of {Adjustment} by {Subclassification} in {Removing} {Bias} in {Observational} {Studies}},
	volume = {24},
	issn = {0006-341X},
	url = {http://www.jstor.org/stable/2528036},
	doi = {10.2307/2528036},
	abstract = {In some investigations, comparison of the means of a variate y in two study groups may be biased because y is related to a variable x whose distribution differs in the two groups. A frequently used device for trying to remove this bias is adjustment by subclassification. The range of x is divided into c subclasses. Weighted means of the subclass means of y are compared, using the same weights for each study group. The effectiveness of this procedure in removing bias depends on several factors, but for monotonic relations between y and x, an analytical approach suggests that for c = 2, 3, 4, 5, and 6 the percentages of bias removed are roughly 64\%, 79\%, 86\%, 90\%, and 92\%, respectively. These figures should also serve as a guide when x is an ordered classification (e.g. none, slight, moderate, severe) that can be regarded as a grouping of an underlying continuous variable. The extent to which adjustment reduces the sampling error of the estimated difference between the y means is also examined. An interesting side result is that for x normal, the percentage reduction in the bias of \${\textbackslash}bar x\_2\$-\${\textbackslash}bar x\_1\$ due to adjustment equals the percentage reduction in its variance. Under a simple mathematical model, errors of measurement in x reduce the amount of bias removed to a fraction 1/(1 + h) of its value, where h is the ratio of the variance of the errors of measurement to the variance of the correct measurements. Since ordered classifications are often used because x is difficult to measure, h may be substantial in such cases, though more information is needed on the values of h that are typical in practice.},
	number = {2},
	urldate = {2016-01-28},
	journal = {Biometrics},
	author = {Cochran, W. G.},
	year = {1968},
	pages = {295--313},
	file = {JSTOR Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/3ZJMN53U/Cochran - 1968 - The Effectiveness of Adjustment by Subclassificati.pdf:application/pdf}
}

@article{abadie_large_2006,
	title = {Large {Sample} {Properties} of {Matching} {Estimators} for {Average} {Treatment} {Effects}},
	volume = {74},
	issn = {1468-0262},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1468-0262.2006.00655.x/abstract},
	doi = {10.1111/j.1468-0262.2006.00655.x},
	abstract = {Matching estimators for average treatment effects are widely used in evaluation research despite the fact that their large sample properties have not been established in many cases. The absence of formal results in this area may be partly due to the fact that standard asymptotic expansions do not apply to matching estimators with a fixed number of matches because such estimators are highly nonsmooth functionals of the data. In this article we develop new methods for analyzing the large sample properties of matching estimators and establish a number of new results. We focus on matching with replacement with a fixed number of matches. First, we show that matching estimators are not N1/2-consistent in general and describe conditions under which matching estimators do attain N1/2-consistency. Second, we show that even in settings where matching estimators are N1/2-consistent, simple matching estimators with a fixed number of matches do not attain the semiparametric efficiency bound. Third, we provide a consistent estimator for the large sample variance that does not require consistent nonparametric estimation of unknown functions. Software for implementing these methods is available in Matlab, Stata, and R.},
	language = {en},
	number = {1},
	urldate = {2016-03-04},
	journal = {Econometrica},
	author = {Abadie, Alberto and Imbens, Guido W.},
	month = jan,
	year = {2006},
	keywords = {Average treatment effects, Matching estimators, potential outcomes, selection on observables, unconfoundedness},
	pages = {235--267},
	file = {Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/M5N68WCN/Abadie and Imbens - 2006 - Large Sample Properties of Matching Estimators for.pdf:application/pdf;Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/4MMHU7N7/abstract.html:text/html}
}

@article{rosenbaum_covariance_2002,
	title = {Covariance {Adjustment} in {Randomized} {Experiments} and {Observational} {Studies}},
	volume = {17},
	issn = {0883-4237, 2168-8745},
	url = {http://projecteuclid.org/euclid.ss/1042727942},
	doi = {10.1214/ss/1042727942},
	abstract = {By slightly reframing the concept of covariance adjustment in randomized experiments, a method of exact permutation inference is derived that is entirely free of distributional assumptions and uses the random assignment of treatments as the "reasoned basis for inference.'' This method of exact permutation inference may be used with many forms of covariance adjustment, including robust regression and locally weighted smoothers. The method is then generalized to observational studies where treatments were not randomly assigned, so that sensitivity to hidden biases must be examined. Adjustments using an instrumental variable are also discussed. The methods are illustrated using data from two observational studies.},
	number = {3},
	urldate = {2016-02-03},
	journal = {Statistical Science},
	author = {Rosenbaum, Paul R.},
	month = aug,
	year = {2002},
	mrnumber = {MR1962487},
	keywords = {Covariance adjustment, matching, observational studies, Permutation inference, propensity score, randomization inference, sensitivity analysis},
	pages = {286--327},
	file = {euclid.ss.1042727942.pdf:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/KBXRSKVX/euclid.ss.1042727942.pdf:application/pdf;Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/GCC3WQ6U/1042727942.html:text/html}
}

@article{abadie_failure_2008,
	title = {On the {Failure} of the {Bootstrap} for {Matching} {Estimators}},
	volume = {76},
	copyright = {© 2008 The Econometric Society},
	issn = {1468-0262},
	url = {http://onlinelibrary.wiley.com/doi/10.3982/ECTA6474/abstract},
	doi = {10.3982/ECTA6474},
	abstract = {Matching estimators are widely used in empirical economics for the evaluation of programs or treatments. Researchers using matching methods often apply the bootstrap to calculate the standard errors. However, no formal justification has been provided for the use of the bootstrap in this setting. In this article, we show that the standard bootstrap is, in general, not valid for matching estimators, even in the simple case with a single continuous covariate where the estimator is root-N consistent and asymptotically normally distributed with zero asymptotic bias. Valid inferential methods in this setting are the analytic asymptotic variance estimator of Abadie and Imbens (2006a) as well as certain modifications of the standard bootstrap, like the subsampling methods in Politis and Romano (1994).},
	language = {en},
	number = {6},
	urldate = {2016-02-19},
	journal = {Econometrica},
	author = {Abadie, Alberto and Imbens, Guido W.},
	month = nov,
	year = {2008},
	keywords = {Average treatment effects, bootstrap, matching},
	pages = {1537--1557},
	file = {Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/GH9ZB466/Abadie and Imbens - 2008 - On the Failure of the Bootstrap for Matching Estim.pdf:application/pdf;Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/7W92QKU9/abstract.html:text/html}
}

@article{matchett_detecting_2015,
	title = {Detecting the influence of rare stressors on rare species in {Yosemite} {National} {Park} using a novel stratified permutation test},
	volume = {5},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/srep10702},
	doi = {10.1038/srep10702},
	urldate = {2016-02-25},
	journal = {Scientific Reports},
	author = {Matchett, J. R. and Stark, Philip B. and Ostoja, Steven M. and Knapp, Roland A. and McKenny, Heather C. and Brooks, Matthew L. and Langford, William T. and Joppa, Lucas N. and Berlow, Eric L.},
	month = jun,
	year = {2015},
	pages = {10702},
	file = {srep10702.pdf:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/A3URRIT8/srep10702.pdf:application/pdf}
}

@article{aronow_cluster-robust_2013,
	title = {Cluster-{Robust} {Variance} {Estimation} for {Dyadic} {Data}},
	url = {http://arxiv.org/abs/1312.3398},
	abstract = {Dyadic data are common in the social sciences, although inference for such settings involves accounting for a complex clustering structure. Many analyses in the social sciences fail to account for the fact that multiple dyads share a member, and that errors are thus likely correlated across these dyads. We propose a nonparametric sandwich-type robust variance estimator for linear regression to account for such clustering in dyadic data. We enumerate conditions for estimator consistency. We also extend our results to repeated and weighted observations, including directed dyads and longitudinal data, and provide an implementation for generalized linear models such as logistic regression. We examine empirical performance with simulations and applications to international relations and speed dating.},
	urldate = {2016-02-26},
	journal = {arXiv:1312.3398 [stat]},
	author = {Aronow, Peter M. and Samii, Cyrus and Assenova, Valentina A.},
	month = dec,
	year = {2013},
	note = {arXiv: 1312.3398},
	keywords = {Statistics - Applications, Statistics - Methodology},
	file = {arXiv\:1312.3398 PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/QUCQK6RE/Aronow et al. - 2013 - Cluster-Robust Variance Estimation for Dyadic Data.pdf:application/pdf;arXiv.org Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/33VDEIXN/1312.html:text/html}
}

@article{aronow_sharp_2014,
	title = {Sharp bounds on the variance in randomized experiments},
	volume = {42},
	issn = {0090-5364, 2168-8966},
	url = {http://projecteuclid.org/euclid.aos/1400592645},
	doi = {10.1214/13-AOS1200},
	abstract = {We propose a consistent estimator of sharp bounds on the variance of the difference-in-means estimator in completely randomized experiments. Generalizing Robins [Stat. Med. 7 (1988) 773–785], our results resolve a well-known identification problem in causal inference posed by Neyman [Statist. Sci. 5 (1990) 465–472. Reprint of the original 1923 paper]. A practical implication of our results is that the upper bound estimator facilitates the asymptotically narrowest conservative Wald-type confidence intervals, with applications in randomized controlled and clinical trials.},
	language = {EN},
	number = {3},
	urldate = {2016-02-26},
	journal = {The Annals of Statistics},
	author = {Aronow, Peter M. and Green, Donald P. and Lee, Donald K. K.},
	month = jun,
	year = {2014},
	mrnumber = {MR3210989},
	zmnumber = {1305.62024},
	keywords = {Causal inference, finite populations, potential outcomes, randomized experiments, variance estimation},
	pages = {850--871},
	file = {Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/IJQFNKGC/1400592645.html:text/html}
}

@article{commenges_transformations_2003,
	title = {Transformations which preserve exchangeability and application to permutation tests},
	volume = {15},
	issn = {1048-5252},
	url = {http://dx.doi.org/10.1080/1048525031000089310},
	doi = {10.1080/1048525031000089310},
	abstract = {Exchangeability of observations is a key condition for applying permutation tests. We characterize the linear transformations which preserve exchangeability, distinguishing second-moment exchangeability and global exchangeability; we also examine non-linear transformations. When exchangeability does not hold one may try to find a transformation which achieves approximate exchangeability; then an approximate permutation test can be done. More specifically, consider a statistic T = φ( Y ); it may be possible to find V such that [Ytilde] = V ( Y ) is exchangeable and to write T = φ¯([Ytilde]). In other cases we may be content that [Ytilde] has an exchangeable variance matrix, which we denote second-moment exchangeability. When seeking transformations towards exchangeability we show the privileged role of residuals. We show that exact permutation tests can be constructed for the normal linear model. Finally we suggest approximate permutation tests based on second-moment exchangeability. In the case of an intraclass correlation model, the transformation is simple to implement. We also give permutational moments of linear and quadratic forms and show how this can be used through Cornish-Fisher expansions.},
	number = {2},
	urldate = {2016-03-01},
	journal = {Journal of Nonparametric Statistics},
	author = {Commenges, Daniel},
	month = jan,
	year = {2003},
	pages = {171--185},
	file = {Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/BAXE2MGJ/Commenges - 2003 - Transformations which preserve exchangeability and.pdf:application/pdf;Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/AI2QXEHS/1048525031000089310.html:text/html}
}

@article{samii_equivalencies_2012,
	title = {On equivalencies between design-based and regression-based variance estimators for randomized experiments},
	volume = {82},
	issn = {0167-7152},
	url = {http://www.sciencedirect.com/science/article/pii/S0167715211003452},
	doi = {10.1016/j.spl.2011.10.024},
	abstract = {This paper demonstrates that the randomization-based “Neyman” and constant-effects estimators for the variance of estimated average treatment effects are equivalent to a variant of the White “heteroskedasticity-robust” estimator and the homoskedastic ordinary least squares (OLS) estimator, respectively.},
	number = {2},
	urldate = {2016-02-26},
	journal = {Statistics \& Probability Letters},
	author = {Samii, Cyrus and Aronow, Peter M.},
	month = feb,
	year = {2012},
	keywords = {potential outcomes, randomized experiments, Robust variance estimators},
	pages = {365--370},
	file = {ScienceDirect Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/P76S8SXI/Samii and Aronow - 2012 - On equivalencies between design-based and regressi.pdf:application/pdf;ScienceDirect Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/WU5C2SDI/S0167715211003452.html:text/html}
}

@article{ding_randomization_2015,
	title = {Randomization inference for treatment effect variation},
	copyright = {© 2015 Royal Statistical Society},
	issn = {1467-9868},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/rssb.12124/abstract},
	doi = {10.1111/rssb.12124},
	abstract = {Applied researchers are increasingly interested in whether and how treatment effects vary in randomized evaluations, especially variation that is not explained by observed covariates. We propose a model-free approach for testing for the presence of such unexplained variation. To use this randomization-based approach, we must address the fact that the average treatment effect, which is generally the object of interest in randomized experiments, actually acts as a nuisance parameter in this setting. We explore potential solutions and advocate for a method that guarantees valid tests in finite samples despite this nuisance. We also show how this method readily extends to testing for heterogeneity beyond a given model, which can be useful for assessing the sufficiency of a given scientific theory. We finally apply our method to the National Head Start impact study, which is a large-scale randomized evaluation of a Federal preschool programme, finding that there is indeed significant unexplained treatment effect variation.},
	language = {en},
	urldate = {2016-03-01},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Ding, Peng and Feller, Avi and Miratrix, Luke},
	month = jul,
	year = {2015},
	keywords = {Causal inference, Head Start, Heterogeneous treatment effect, Randomization test},
	pages = {n/a--n/a},
	file = {Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/UD4S9IFJ/Ding et al. - 2015 - Randomization inference for treatment effect varia.pdf:application/pdf;Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/PQUP882D/abstract.html:text/html}
}

@article{newey_asymptotic_1994,
	title = {The {Asymptotic} {Variance} of {Semiparametric} {Estimators}},
	volume = {62},
	issn = {0012-9682},
	url = {http://www.jstor.org/stable/2951752},
	doi = {10.2307/2951752},
	abstract = {The purpose of this paper is the presentation of a general formula for the asymptotic variance of a semiparametric estimator. A particularly important feature of this formula is a way of accounting for the presence of nonparametric estimates of nuisance functions. The general form of an adjustment factor for nonparametric estimates is derived and analyzed. The usefulness of the formula is illustrated by deriving propositions on invariance of the limiting distribution with respect to the nonparametric estimator, conditions for nonparametric estimation to have no effect on the asymptotic distribution, and the form of a correction term for the presence of nonparametric projection and density estimators. Examples discussed are quasi-maximum likelihood estimation of index models, panel probit with semiparametric individual effects, average derivatives, and inverse density weighted least squares. The paper also develops a set of regularity conditions for the validity of the asymptotic variance formula. Primitive regularity conditions are derived for {\textless}tex-math{\textgreater}\${\textbackslash}sqrt\{n\}{\textbackslash}text\{-consistency\}\${\textless}/tex-math{\textgreater} and asymptotic normality for functions of series estimators of projections. Specific examples are polynomial estimators of average derivative and semiparametric panel probit models.},
	number = {6},
	urldate = {2016-03-03},
	journal = {Econometrica},
	author = {Newey, Whitney K.},
	year = {1994},
	pages = {1349--1382},
	file = {JSTOR Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/XGFWEAT3/Newey - 1994 - The Asymptotic Variance of Semiparametric Estimato.pdf:application/pdf}
}

@article{austin_comparison_2014,
	title = {A comparison of 12 algorithms for matching on the propensity score},
	volume = {33},
	issn = {1097-0258},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/sim.6004/abstract},
	doi = {10.1002/sim.6004},
	abstract = {Propensity-score matching is increasingly being used to reduce the confounding that can occur in observational studies examining the effects of treatments or interventions on outcomes. We used Monte Carlo simulations to examine the following algorithms for forming matched pairs of treated and untreated subjects: optimal matching, greedy nearest neighbor matching without replacement, and greedy nearest neighbor matching without replacement within specified caliper widths. For each of the latter two algorithms, we examined four different sub-algorithms defined by the order in which treated subjects were selected for matching to an untreated subject: lowest to highest propensity score, highest to lowest propensity score, best match first, and random order. We also examined matching with replacement. We found that (i) nearest neighbor matching induced the same balance in baseline covariates as did optimal matching; (ii) when at least some of the covariates were continuous, caliper matching tended to induce balance on baseline covariates that was at least as good as the other algorithms; (iii) caliper matching tended to result in estimates of treatment effect with less bias compared with optimal and nearest neighbor matching; (iv) optimal and nearest neighbor matching resulted in estimates of treatment effect with negligibly less variability than did caliper matching; (v) caliper matching had amongst the best performance when assessed using mean squared error; (vi) the order in which treated subjects were selected for matching had at most a modest effect on estimation; and (vii) matching with replacement did not have superior performance compared with caliper matching without replacement. © 2013 The Authors. Statistics in Medicine published by John Wiley \& Sons, Ltd.},
	language = {en},
	number = {6},
	urldate = {2016-03-05},
	journal = {Statistics in Medicine},
	author = {Austin, Peter C.},
	month = mar,
	year = {2014},
	keywords = {computer algorithms, matching, Monte Carlo simulations, optimal matching, propensity score, propensity-score matching},
	pages = {1057--1069},
	file = {Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/KIHM2JE3/Austin - 2014 - A comparison of 12 algorithms for matching on the .pdf:application/pdf;Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/DDF68PBF/abstract.html:text/html}
}

@article{freedman_nonstochastic_1983,
	title = {A {Nonstochastic} {Interpretation} of {Reported} {Significance} {Levels}},
	volume = {1},
	issn = {0735-0015},
	url = {http://www.jstor.org/stable/1391660},
	doi = {10.2307/1391660},
	abstract = {Tests of significance are often made in situations where the standard assumptions underlying the probability calculations do not hold. As a result, the reported significance levels become difficult to interpret. This article sketches an alternative interpretation of a reported significance level, valid in considerable generality. This level locates the given data set within the spectrum of other data sets derived from the given one by an appropriate class of transformations. If the null hypothesis being tested holds, the derived data sets should be equivalent to the original one. Thus, a small reported significance level indicates an unusual data set. This development parallels that of randomization tests, but there is a crucial technical difference: our approach involves permuting observed residuals; the classical randomization approach involves permuting unobservable, or perhaps nonexistent, stochastic disturbance terms.},
	number = {4},
	urldate = {2016-03-07},
	journal = {Journal of Business \& Economic Statistics},
	author = {Freedman, David and Lane, David},
	year = {1983},
	pages = {292--298},
	file = {JSTOR Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/3PRPMG5F/Freedman and Lane - 1983 - A Nonstochastic Interpretation of Reported Signifi.pdf:application/pdf}
}

@article{wager_estimation_2015,
	title = {Estimation and {Inference} of {Heterogeneous} {Treatment} {Effects} using {Random} {Forests}},
	url = {http://arxiv.org/abs/1510.04342},
	abstract = {Many scientific and engineering challenges---ranging from personalized medicine to customized marketing recommendations---require an understanding of treatment effect heterogeneity. In this paper, we develop a non-parametric causal forest for estimating heterogeneous treatment effects that extends Breiman's widely used random forest algorithm. Given a potential outcomes framework with unconfoundedness, we show that causal forests are pointwise consistent for the true treatment effect, and have an asymptotically Gaussian and centered sampling distribution. We also discuss a practical method for constructing asymptotic confidence intervals for the true treatment effect that are centered at the causal forest estimates. Our theoretical results rely on a generic Gaussian theory for a large family of random forest algorithms, to our knowledge, this is the first set of results that allows any type of random forest, including classification and regression forests, to be used for provably valid statistical inference. In experiments, we find causal forests to be substantially more powerful than classical methods based on nearest-neighbor matching, especially as the number of covariates increases.},
	urldate = {2016-03-10},
	journal = {arXiv:1510.04342 [math, stat]},
	author = {Wager, Stefan and Athey, Susan},
	month = oct,
	year = {2015},
	note = {arXiv: 1510.04342},
	keywords = {Mathematics - Statistics Theory, Statistics - Machine Learning, Statistics - Methodology},
	file = {arXiv\:1510.04342 PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/FDCDMREZ/Wager and Athey - 2015 - Estimation and Inference of Heterogeneous Treatmen.pdf:application/pdf;arXiv.org Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/ED79SEEV/1510.html:text/html}
}

@article{athey_recursive_2015,
	title = {Recursive {Partitioning} for {Heterogeneous} {Causal} {Effects}},
	url = {http://arxiv.org/abs/1504.01132},
	abstract = {In this paper we study the problems of estimating heterogeneity in causal effects in experimental or observational studies and conducting inference about the magnitude of the differences in treatment effects across subsets of the population. In applications, our method provides a data-driven approach to determine which subpopulations have large or small treatment effects and to test hypotheses about the differences in these effects. For experiments, our method allows researchers to identify heterogeneity in treatment effects that was not specified in a pre-analysis plan, without concern about invalidating inference due to multiple testing. In most of the literature on supervised machine learning (e.g. regression trees, random forests, LASSO, etc.), the goal is to build a model of the relationship between a unit's attributes and an observed outcome. A prominent role in these methods is played by cross-validation which compares predictions to actual outcomes in test samples, in order to select the level of complexity of the model that provides the best predictive power. Our method is closely related, but it differs in that it is tailored for predicting causal effects of a treatment rather than a unit's outcome. The challenge is that the "ground truth" for a causal effect is not observed for any individual unit: we observe the unit with the treatment, or without the treatment, but not both at the same time. Thus, it is not obvious how to use cross-validation to determine whether a causal effect has been accurately predicted. We propose several novel cross-validation criteria for this problem and demonstrate through simulations the conditions under which they perform better than standard methods for the problem of causal effects. We then apply the method to a large-scale field experiment re-ranking results on a search engine.},
	urldate = {2016-03-10},
	journal = {arXiv:1504.01132 [stat]},
	author = {Athey, Susan and Imbens, Guido},
	month = apr,
	year = {2015},
	note = {arXiv: 1504.01132},
	keywords = {Statistics - Machine Learning},
	file = {arXiv\:1504.01132 PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/SPBKADRE/Athey and Imbens - 2015 - Recursive Partitioning for Heterogeneous Causal Ef.pdf:application/pdf;arXiv.org Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/5WH9I8G3/1504.html:text/html}
}

@article{sloczynski_general_????,
	title = {A {General} {Weighted} {Average} {Representation} of the {Ordinary} and {Two}-{Stage} {Least} {Squares} {Estimands}},
	abstract = {It is standard practice in applied work to study the effect of a binary vari- able (“treatment”) on some outcome of interest using simple linear models with homogeneous effects. In this paper I study the interpretation of the ordi- nary and two-stage least squares estimands in such models when treatment effects are in fact heterogeneous. I show that in both cases the coefficient on treatment is identical to the outcome of the following three-step proce- dure: first, calculate the linear projection of treatment on the vector of other covariates (“propensity score”); second, calculate average partial effects for both groups of interest (“treated” and “controls”) from a regression of out- come on treatment, the propensity score, and their interaction; third, calculate a weighted average of these two effects, with weights being inversely related to the unconditional probability that a unit belongs to a given group. The only difference between the ordinary and two-stage least squares estimands is in adding residuals from a linear first stage to the vector of other covariates when computing the propensity scores in the latter case. Importantly, the re- liance on these implicit weights—which are inversely related to the proportion of each group—can have severe consequences for applied work. To illustrate the importance of this result, I perform Monte Carlo simulations and replicate three recent applied papers: Dinkelman (2011), Berger, Easterly, Nunn and Satyanath (2013), and Martinez-Bravo (2014). In each of these cases some of the conclusions change dramatically after allowing for heterogeneity in effects.},
	author = {Sloczynski, Tymon},
	file = {Sloczynski_paper_regression.pdf:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/WIA8ICX4/Sloczynski_paper_regression.pdf:application/pdf}
}

@article{aronow_does_2016,
	title = {Does {Regression} {Produce} {Representative} {Estimates} of {Causal} {Effects}?},
	volume = {60},
	copyright = {© 2015 by the Midwest Political Science Association},
	issn = {1540-5907},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/ajps.12185/abstract},
	doi = {10.1111/ajps.12185},
	abstract = {With an unrepresentative sample, the estimate of a causal effect may fail to characterize how effects operate in the population of interest. What is less well understood is that conventional estimation practices for observational studies may produce the same problem even with a representative sample. Causal effects estimated via multiple regression differentially weight each unit's contribution. The “effective sample” that regression uses to generate the estimate may bear little resemblance to the population of interest, and the results may be nonrepresentative in a manner similar to what quasi-experimental methods or experiments with convenience samples produce. There is no general external validity basis for preferring multiple regression on representative samples over quasi-experimental or experimental methods. We show how to estimate the “multiple regression weights” that allow one to study the effective sample. We discuss alternative approaches that, under certain conditions, recover representative average causal effects. The requisite conditions cannot always be met.},
	language = {en},
	number = {1},
	urldate = {2016-03-29},
	journal = {American Journal of Political Science},
	author = {Aronow, Peter M. and Samii, Cyrus},
	month = jan,
	year = {2016},
	pages = {250--267},
	file = {Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/S6KTC5US/Aronow and Samii - 2016 - Does Regression Produce Representative Estimates o.pdf:application/pdf;Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/4IT7PQWJ/abstract\;jsessionid=40863215CA1054D798558376F2E23AEB.html:text/html}
}

@article{hansen_prognostic_2008,
	title = {The prognostic analogue of the propensity score},
	volume = {95},
	issn = {0006-3444, 1464-3510},
	url = {http://biomet.oxfordjournals.org/content/95/2/481},
	doi = {10.1093/biomet/asn004},
	abstract = {The propensity score collapses the covariates of an observational study into a single measure summarizing their joint association with treatment conditions; prognostic scores summarize covariates' association with potential responses. As with propensity scores, stratification on prognostic scores brings to uncontrolled studies a concrete and desirable form of balance, a balance that is more familiar as an objective of experimental control. Like propensity scores, prognostic scores can reduce the dimension of the covariate, yet causal inferences conditional on them are as valid as are inferences conditional only on the unreduced covariate. As a method of adjustment unto itself, prognostic scoring has limitations not shared with propensity scoring, but it holds promise as a complement to the propensity score, particularly in certain designs for which unassisted propensity adjustment is difficult or infeasible.},
	language = {en},
	number = {2},
	urldate = {2016-03-16},
	journal = {Biometrika},
	author = {Hansen, Ben B.},
	month = jun,
	year = {2008},
	keywords = {Covariate balance, Matched sampling, matching, Observational study, Quasi-experiment, Regression discontinuity, Subclassification},
	pages = {481--488},
	file = {Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/SUCQWJH8/Hansen - 2008 - The prognostic analogue of the propensity score.pdf:application/pdf;Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/2CI6X846/481.html:text/html}
}

@article{lee_training_2009,
	title = {Training, {Wages}, and {Sample} {Selection}: {Estimating} {Sharp} {Bounds} on {Treatment} {Effects}},
	volume = {76},
	issn = {0034-6527},
	shorttitle = {Training, {Wages}, and {Sample} {Selection}},
	url = {http://www.jstor.org/stable/40247633},
	abstract = {This paper empirically assesses the wage effects of the Job Corps program, one of the largest federally funded job training programs in the U.S. Even with the aid of a randomized experiment, the impact of a training program on wages is difficult to study because of sample selection, a pervasive problem in applied microeconometric research. Wage rates are only observed for those who are employed, and employment status itself may be affected by the training program. This paper develops an intuitive trimming procedure for bounding average treatment effects in the presence of sample selection. In contrast to existing methods, the procedure requires neither exclusion restrictions nor a bounded support for the outcome of interest. Identification results, estimators, and their asymptotic distribution are presented. The bounds suggest that the program raised wages, consistent with the notion that the Job Corps raises earnings by increasing human capital, rather than solely through encouraging work. The estimator is generally applicable to typical treatment evaluation problems in which there is nonrandom sample selection/ attrition.},
	number = {3},
	urldate = {2016-03-17},
	journal = {The Review of Economic Studies},
	author = {Lee, David S.},
	year = {2009},
	pages = {1071--1102},
	file = {JSTOR Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/SSGKRJFP/Lee - 2009 - Training, Wages, and Sample Selection Estimating .pdf:application/pdf}
}