\documentclass[12pt]{article}
\usepackage[margin=0.75in]{geometry}
%\usepackage[breaklinks=true]{hyperref}
%\usepackage[html,png]{tex4ht}
\usepackage{color}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{natbib}
%\usepackage{array}
%\usepackage{booktabs, multicol, multirow}
%\usepackage[nohead]{geometry}
\usepackage[singlespacing]{setspace}
\setlength{\parindent}{0cm}
%\usepackage[bottom]{footmisc}
%\usepackage{floatrow}
%\usepackage{float}
%\usepackage{caption}
%\usepackage{indentfirst}
%\usepackage{lscape}
%\usepackage{floatrow}
%\usepackage{epsfig}
%\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage{hyperref}
%\floatsetup[table]{capposition=top}
%\floatsetup[figure]{capposition=top}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}

\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}

\newcommand{\cD}{{\mathcal D}}
\newcommand{\cF}{{\mathcal F}}
\newcommand{\todo}[1]{{\color{red}{TO DO: \sc #1}}}

\newcommand{\reals}{\mathbb{R}}
\newcommand{\integers}{\mathbb{Z}}
\newcommand{\naturals}{\mathbb{N}}
\newcommand{\rationals}{\mathbb{Q}}

\newcommand{\ind}[1]{1_{#1}} % Indicator function
\newcommand{\pr}{\mathbb{P}} % Generic probability
\newcommand{\ex}{\mathbb{E}} % Generic expectation
\newcommand{\var}{\textrm{Var}}
\newcommand{\cov}{\textrm{Cov}}

\newcommand{\normal}{N} % for normal distribution (can probably skip this)
\newcommand{\eps}{\varepsilon}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

% Convergence
\newcommand{\convd}{\stackrel{d}{\longrightarrow}} % convergence in distribution/law/measure
\newcommand{\convp}{\stackrel{P}{\longrightarrow}} % convergence in probability
\newcommand{\convas}{\stackrel{\textrm{a.s.}}{\longrightarrow}} % convergence almost surely

\newcommand{\eqd}{\stackrel{d}{=}} % equal in distribution/law/measure
\newcommand{\argmax}{\textrm{argmax}}
\newcommand{\argmin}{\textrm{argmin}}
\renewcommand{\baselinestretch}{1.5}

\title{Model-based matching}
\author{Kellie Ottoboni}
\date{Draft \today}

\begin{document}
\maketitle


\begin{abstract}
Drawing causal inferences from nonexperimental data is difficult due to the presence of confounders, variables that affect both the selection into treatment groups and the outcome. Post-hoc matching and stratification can be used to group individuals who are comparable with respect to important variables, but commonly used methods often fail to balance confounders between groups. We introduce model-based matching, a nonparametric method which groups observations that would be alike aside from the treatment. We use model-based matching to conduct stratified permutation tests of association between the treatment and outcome, controlling for other variables. Under standard assumptions from the causal inference literature, model-based matching can be used to estimate average treatment effects.
\end{abstract}

\newpage
\section{Introduction}
% https://docs.google.com/document/d/1En14zONJR0DHBxewCghHFbRep70vTlnKYRBlzoasCZ8/edit#

Observational studies in a range of fields including social sciences, epidemiology, and ecology are used to make inferences about cause and effect.  Causal inference can be viewed as a missing data problem: one is only able to see each individual's outcome after treatment or no treatment, but not both (Holland, 1984).  To estimate the effect of treatment, one must use a control group as the counterfactual.  The treatment effect is obscured by confounders, variables that are entangled with both the treatment and outcome.  In an ideal situation, to adjust for the effect of confounders one would estimate the difference in outcomes between cases and controls who are identical with respect to all confounders, then average over the pairs.  \\

In practice, the curse of dimensionality makes this impossible: studies typically account for a large number of covariates, so groups of individuals matched exactly on all pretreatment covariates can be too small to provide adequate statistical power to detect a significant treatment effect. For example, it is difficult to study the effect of a job-training program on income levels when the participants differ from non-participants on a wide range of socioeconomic variables, as well as potential unmeasured confounders (Dehejia \& Wahba, 1999).  Post hoc methods to construct a group of controls whose covariates balance the covariates in the treatment group include exact matching on covariates, matching and weighting by propensity score (Rosenbaum \& Rubin, 1983), genetic matching (Diamond \& Sekhon, 2013), and maximum-entropy weighting (Hainmueller, 2012).  These balanced control groups are then used to estimate average treatment effects using standard methods such as unadjusted differences of means or linear regression controlling for other covariates.  When matching and weighting methods fail to achieve balance in all the pretreatment covariates, estimates of the average treatment effect can be severely biased (Freedman \& Berk, 2008).  \\

The proposed method improves upon existing methods by eliminating the need for parametric assumptions such as Gaussian and homoscedastic errors, linearity, and adequate support.  Observational studies often violate these key assumptions, so model-based matching may better accommodate real-world data from observational studies than traditional methods.  Additionally, it is more flexible than traditional methods that assume that the treatment is binary and outcome is continuous; by modifying the statistic used to measure the strength of association, one may use categorical or continuous treatment and outcome variables.  

The method has been applied before in a study of the effect of packstock use on the amphibian population in Yosemite National Park (Matchett, Stark, et. al 2015). % http://www.nature.com/articles/srep10702
In this paper, we develop the theory behind the testing method and discuss estimation strategies.

\subsection{Notation}
Let $Y_i(0)$ and $Y_i(1)$ be individual $i$'s potential outcomes under control and to treatment, respectively.
$T_i$ is the treatment assigned to individual $i$.
Then the observed outcome is $Y_i = Y_i(1)T_i + Y_i(0)(1-T_i)$.

A standard assumption is exogeneity, or conditional independence:  $(Y(0), Y(1)) \independent T \mid X$.




\section{Estimation}
We begin this section with a review of matching and stratification estimators.
Next, we present the proposed estimator and show under what conditions it is unbiased for the average treatment effect.
Finally, we show that a special case of the proposed estimator has smaller variance than the usual difference in means estimator in a completely randomized experiment.

\subsection{Matching and Stratified Estimators}
\todo{ Do a lit review of matching here}

In randomized control trials, the potential outcomes are balanced between treatment and control groups by construction: in other words, $(Y(0), Y(1)) \independent T$.
In observational studies, this is no longer guaranteed.
Rosenbaum and Rubin (1983) achieve a weaker form of this balance by appealing to the propensity score, $p(x) = \pr(T = 1 \mid X = x)$.
The propensity score is a balancing score, in the sense that $X \independent T \mid p(X)$.
Under the assumptions of conditional independence given $X$ and overlap in the distribution of propensity scores (together, called ``strong ignorability''), they show that strong ignorability given $X$ implies strong ignorability given $p(X)$, and that by the law of iterated expectations, we can recover the overall average treatment effect.

One issue is that in observational studies, the propensity score is unknown.
One typically estimates the propensity score using logistic or probit regression models using a set of observed covariates.
When the propensity score estimates are wrong, then estimates of the average treatment effect can be biased.

We'd like to try to achieve balance in the potential outcomes some other way:
$(Y(0), Y(1)) \independent T \mid \hat{Y}$, where $\hat{Y}$ is the model-based prediction of $Y$, absent knowledge of the treatment,  for all units.

\subsection{Stratified Estimation}

Recall that

$$ATE = \ex(Y_1 -Y_0 )$$

Let $\hat{Y} = f(X)$ be a prediction of the response under the control regime. \todo{ mention fitting f problem}
\citet{hansen_prognostic_2008} defines a prognostic score as a function $\Psi(X)$ such that $Y(0) \independent X \mid \Psi(X)$ for all $X$.
That is, conditioning on a prognostic score induces balance in the covariate distributions of individuals with contrasting potential outcomes.
\citet{hansen_prognostic_2008} shows the following useful result:

\begin{lemma}
Suppose that there is no hidden bias, so $(Y(0), Y(1)) \independent T \mid X$. Then conditioning on a prognostic score deconfounds potential response under the control regime from treatment assignment:

$$Y(0) \independent T \mid \Psi(X)$$

Furthermore, if there is no effect modification, then

$$Y(1) \independent T \mid \Psi(X)$$

\end{lemma}

We omit the details of effect modification and the proof.



If there is no hidden bias, then $\hat{Y}$ is a prognostic score.
Matching individuals exactly on $\hat{Y}$ presents the same issues as matching on the propensity score.
Instead, we propose stratifying based on $\hat{Y}$.
Let $\Psi_s(X)$ be a function that classifies individuals into one of $S$ strata based on their predicted control potential outcome.
That is, $\Psi_s(X) = \Pi(\hat{Y}) = \Pi(f(X))$.
The stratification rule $\Psi_s$ and estimation procedure for $\hat{Y}$ are defined independently of the observed sample.
Define $S_i = \Psi_s(X_i)$ to be the stratum assignment of unit $i$.
The $s$th stratum contains $N_{st}$ treated individuals and $N_{sc}$ control individuals for a total of $N_s = N_{st}+N_{sc}$ individuals, so $N = N_1 + \dots + N_S$.
We estimate the ATE by


\begin{equation}\label{tau_hat}
\hat{\tau} =  \sum_{s=1}^{S} \frac{N_S}{N}\left( \frac{1}{N_{st}} \sum_{i: T_i=1, S_i=s} (Y_i - \hat{Y}_i) - \frac{1}{N_{sc}} \sum_{i: T_i=0, S_i=s} (Y_i - \hat{Y}_i)\right)
\end{equation}

$\hat{\tau}$ is the weighted average of within-stratum estimated treatment effects, where weights are proportional to the stratum sizes.
\todo{clean up this paragraph}
For this estimator to ``work'', we need the stratum-specific estimators to be unbiased.
A sufficient condition is conditional independence between potential outcomes and treatment given stratum assignment: $(Y(0), Y(1)) \independent T \mid S$.
The stratification collapses redundant information in $\hat{Y}$ but must preserve the relevant information about how covariates relate to treatment assignment.
This condition implies that $\pr(T_i = 1 \mid S_i = s, Y_i(1), Y_i(0)) = \pr(T_i = 1 \mid S_i = s) $, so the probability of any particular unit receiving treatment is constant within strata.
Intuitively, treatment is ``as if random'' within strata.
In a randomized experiment, the analogous condition is that the propensity score is constant within strata, i.e. treatment \textit{is} assigned at random within strata.

We prove a useful lemma before our main result.

\begin{lemma}
If $(Y(0), Y(1)) \independent T \mid S$ and treatment is assigned independently across units within stratum $s$, then 
$$\ex\left( \frac{T_i}{N_{st}} \mid S_i =s, \sum_{i}\ind{S_i = s} = N_s\right) = \frac{1}{N_s}$$
Similarly, $\ex\left(\frac{1-T_i}{N_{sc}} \mid S_i = s, \sum_{i}\ind{S_i = s} = N_s\right) = \frac{1}{N_s}$.
\end{lemma}

\begin{proof}
\begingroup
\addtolength{\jot}{-0.5em}
\begin{align*}
\ex\left( \frac{T_i}{N_{st}} \mid S_i = s, \sum_{i}\ind{S_i = s} = N_s \right) &= \ex\left( \frac{1}{N_{st}} \ex(T_i \mid N_{st}, S_i = s, \sum_{i}\ind{S_i = s} = N_s) \right) \\
&= \ex\left( \frac{1}{N_{st}} \frac{N_{st}}{N_s}\right) \\
&= \frac{1}{N_s}
\end{align*}
\endgroup
\end{proof}

If  $\hat{\tau}$ is unbiased for the ATE:

\begin{theorem}
If $Y(1), Y(0) \independent T \mid S$ and $0 < N_{st} < N_s$ for $s = 1, \dots, S$, then $\hat{\tau}$ is unbiased for the ATE.
\end{theorem}

\begin{proof}
Conditional on $N_1, \dots, N_S$ we have
\begin{align*}
\ex(\hat{\tau}) &= \ex\left[ \sum_{s=1}^{S} \frac{N_S}{N}\left( \frac{1}{N_{st}} \sum_{i: T_i=1, S_i=s} (Y_i - \hat{Y}_i) - \frac{1}{N_{sc}} \sum_{i: T_i=0, S_i=s} (Y_i - \hat{Y}_i)\right) \right] \\
&= \ex\left[ \sum_{s=1}^{S} \frac{N_s}{N}\left( \frac{1}{N_s} \sum_{i: S_i=s} \frac{T_i(Y_i(1) - \hat{Y}_i)}{N_{st}/N_s} - \frac{(1-T_i)(Y_i(0) - \hat{Y}_i)}{N_{sc}/N_s}\right) \right] \\
&=\ex\left[  \sum_{s=1}^{S} \frac{1}{N}\left( \sum_{i: S_i=s} \ex\left(\frac{T_i}{N_{st}/N_s} \mid S_i = s\right)\ex(Y_i(1) - \hat{Y}_i \mid S_i=s) \right.\right. \\
& \qquad\left.\left. - \ex\left(\frac{1-T_i}{N_{sc}/N_s} \mid S_i=s\right)\ex(Y_i(0) - \hat{Y}_i \mid S_i=s) \right) \right] \\
&= \ex\left[ \sum_{s=1}^{S} \frac{1}{N}\left( \sum_{i: S_i=s} \frac{1/N_s}{1/N_s}\ex(Y_i(1) - \hat{Y}_i \mid S_i=s) - \frac{1/N_s}{1/N_s}\ex(Y_i(0) - \hat{Y}_i \mid S_i=s)\right)\right] \\
&= \ex\left[ \sum_{s=1}^{S} \frac{1}{N}\left( \sum_{i: S_i=s} \ex(Y_i(1) - \hat{Y}_i \mid S_i=s) - \ex(Y_i(0) - \hat{Y}_i \mid S_i=s)\right)\right] \\
&= \ex\left[ \frac{1}{N}\sum_{i=1}^N \ex\left(Y_i(1)- Y_i(0) \mid S_i =s \right)\right] \\
&= ATE
\end{align*}
Then, taking expectations with respect to $N_1, \dots, N_S$, we have that $\ex(\hat{\tau}) = ATE$ unconditionally as well.
\end{proof}




\subsubsection{Variance in a randomized experiment}
Assume that we sample $N$ units from a super population and do a complete randomization with $N_t$ units assigned to treatment.  
Instead of the stratified estimator used above, consider the special case of a single stratum:

$$\hat{\tau}^{adj} = \frac{1}{N_t} \sum_{i: T_i=1} (Y_i - \hat{Y}_i) - \frac{1}{N_c} \sum_{i: T_i = 0} (Y_i - \hat{Y}_i)$$

Randomization ensures that this is an unbiased estimate of the ATE.

Define $\sigma_1^2 = \var_{SP}(Y(1))$, $\sigma_0^2 = \var_{SP}(Y(0))$, and $\sigma_{01}^2 = \var_{SP}(Y(1) - Y(0)) = \cov_{SP}(Y(1), Y(0))$.  In addition, define $\nu^2 = \var_{SP}(\hat{Y})$, $\rho_1 = \frac{\cov_{SP}(Y(1), \hat{Y})}{\sigma_1\nu}$, and $\rho_0 = \frac{\cov_{SP}(Y(0), \hat{Y})}{\sigma_0\nu}$.

\begin{theorem}\label{thm:var_tau_hat_adjusted}
\begin{equation}\label{var_tau_hat_adjusted}
\var(\hat{\tau}^{adj}) = \frac{(\sigma_1 - \nu)^2 - 2(1-\rho_1)\sigma_1\nu}{N_t} + \frac{ (\sigma_0 - \nu)^2 - 2(1-\rho_0)\sigma_0\nu}{N_c}
\end{equation}
\end{theorem}

The proof appears in the appendix.




Compare this to  $\hat{\tau}^{diff}$, the usual difference in means estimator.  In our simulations, we observe that $\var(\hat{\tau}^{adj})$ is an order of magnitude smaller than $\var\left(\hat{\tau}^{diff}\right)$.  Using the same proof as above but substituting $Y$ for $r$, one can show that the variance of $\hat{\tau}^{diff}$ is

\begin{equation}
\var\left(\hat{\tau}^{diff}\right) = \frac{\sigma_1^2}{N_t} + \frac{\sigma_0^2}{N_c}
\end{equation}

To guarantee $\var(\hat{\tau}^{adj}) < \var(\hat{\tau}^{diff})$, it is sufficient to have $\sigma_1 \geq \frac{\nu}{2(2-\rho_1)}$ and $\sigma_0 \geq \frac{\nu}{2(2-\rho_0)}$.  
Indeed, this always holds true when the variance in predictions is no more than four times the variance in outcomes.  
This will hold for any reasonable predictor $\hat{Y}$.
Therefore, we conclude that residualizing improves precision.

Does stratification improve precision beyond this?
Analytically, the variance of $\hat{\tau}$ from Equation~\ref{tau_hat} depends on the number of strata and how they partition the covariate space.
While the within-stratum estimated treatment effects may have smaller variances, their covariance will generally be positive and thus inflate the overall variance.
In the next section, we find empirically that \todo


\newpage
\subsection{Empirical Results}




\section{Hypothesis Testing}

\subsection{Tests of Residuals}
Tests of residuals after covariance adjustment appear in various forms in the literature.
Rosenbaum (2002) uses residuals after fitting prediction models to stabilize estimates of treatment effects for more powerful randomization tests.
Rosenbaum's framework is limited to the case of binary treatment, where individuals are either assigned to treatment or receive the control.

Shah and Bühlmann (2015) use residuals to test for the goodness of fit of high-dimensional linear models by testing for nonlinear signals in the residuals. %http://www.statslab.cam.ac.uk/~rds37/papers/RPtests


\subsection{Hypothesis testing in randomized experiments}

Suppose we observe $N$ individuals with responses $Y_1, \dots, Y_N$.  Assume that their response takes the form

\begin{equation}
Y_i(t) = f(t, X_i) + \eps_i
\end{equation}
where $X_i$ is a vector of covariates for individual $i$, $t$ is the treatment received, and $\eps_i$ is random error.  Assume that the $\eps_i$ are independent and identically distributed, with $\ex(\eps_i) = 0$.
In a randomized experiment, the $N$ individuals are assigned treatment at random.
$T_i$ is an indicator for whether individual $i$ received treatment, with $T_i \independent (X_i, \eps_i)$.

Suppose we'd like to test the strong null hypothesis of no treatment effect, which implies that $Y_i(0) = Y_i(1)$.
We'd test the null by computing some test statistic of the responses $Y = Y(T) = (Y_1(T_1), \dots, Y_N(T_N))$, and treatment assignment, $T = (T_1, \dots, T_N)$, as
$\tau(Y, T)$.
Note that under the strong null, $Y$ does not actually depend on $T$, and so for any permutation $T^*$ of treatment assignments, $Y(T) = Y(T^*)$.
Suppose we take $K$ strata of observations.
We would generate a null distribution for $\tau$ by permuting the treatment assignments within the $K$ strata to obtain an approximate distribution $\tau(Y, T^*_1), \dots, \tau(Y,T^*_B)$ for some large $B$ and compare our observed test statistic to the distribution to obtain a p-value.
Extreme values of $\tau(Y, T)$ give evidence for rejecting the null hypothesis.

We would like to stabilize the variance of $Y$ by removing some extra variance coming from known covariates $X_i$.
In particular, under the strong null, $f(0, x) = f(1, x)$ for all $x$.
Since the errors $\eps_i$ are IID, the values $Y_i(t) - f(t, X_i)$ are also IID (and thus exchangeable).
However, $f$ is unobserved so we must estimate it.
Since $f$ is constant with respect to treatment, we may estimate it as a function of $x$ alone.
We'd like to use the residuals $e_i = Y_i(t) - \hat{f}(X_i)$ in place of the unobserved errors $\eps_i$.
Under random assignment and IID assumptions, $e_i$ are also 




\subsection{Hypothesis testing in observational studies}

To ensure that the residuals are exchangeable, we need them to be some exchangeable function of $\eps_i$ within each stratum, e.g. if observation $i$ belongs to the $k$th stratum, then $e_i = g_k(\eps_i)$.
\textcolor{red}{What are the conditions on the $g_k$ that ensure $e_i$ are exchangeable?}
It is sufficient to have $g_k$ be a strictly monotonic function, such as $g_k(\eps) = \alpha_k + \beta_k \eps$.


\textcolor{red}{If exchangeability does not hold, we want to find a transformation $\phi$ such that $\phi(e_i)$ are exchangeable.}


\subsection{Empirical Results}




\section{Discussion}

\newpage
\section{Appendix}


\begin{proof}[Proof of Theorem~\ref{thm:var_tau_hat_adjusted}]
Define $r_i = Y_i - \hat{Y}_i$ to be the observed residualized outcome in the sample.  We rewrite the estimator as

$$\hat{\tau}^{adj} = \overline{r_t} - \overline{r_c}$$

where $\overline{r_t}$ and $\overline{r_c}$ are the average residualized outcomes in the treatment and control groups, respectively.  The residualized potential outcomes are then defined as $r_i(1) = Y_i(1) - \hat{Y}_i$ and $r_i(0) = Y_i(0) - \hat{Y}_i$.  Let $\overline{r_i(1)}$ and $\overline{r_i(0)}$ be the average residualized potential outcomes in the sample and $\overline{R_i(1)}$ and $\overline{R_i(0)}$ be the analogous population quantities: that is, $\ex_{SP}(r_i(1)) = \overline{R_i(1)}$ and $\ex_{SP}(r_i(0)) = \overline{R_i(0)}$.


\begin{align*}
\var(\hat{\tau}^{adj}) &= \var\left( \overline{r_t} - \overline{r_c} \right) \\
&= \ex\left[ \left(  \overline{r_t} - \overline{r_c} - \ex_{SP}\left[ r(1) - r(0) \right] \right)^2 \right] \\
&= \ex\left[ \left(  \overline{r_t} - \overline{r_c} - \left(\overline{r(1)} - \overline{r(0)}\right) + \left(\overline{r(1)} - \overline{r(0)}\right) -  \ex_{SP}\left[ r(1) - r(0) \right] \right)^2 \right] \\
&= \ex\left[  \left(  \overline{r_t} - \overline{r_c} - \left(\overline{r(1)} - \overline{r(0)}\right) \right)^2\right] + \ex\left[ \left( \left(\overline{r(1)} - \overline{r(0)}\right) -  \ex_{SP}\left[ r(1) - r(0) \right] \right)^2 \right] \\
&\qquad\qquad + 2 \ex\left[  \left(  \overline{r_t} - \overline{r_c} - (\overline{r(1)} - \overline{r(0)}) \right)\left( (\overline{r(1)} - \overline{r(0)}) -  \ex_{SP}\left[ r(1) - r(0) \right] \right) \right]
\end{align*}

Note that the expectations above are taken over both the random sampling from the superpopulation and the random assignment of treatments.  The third term is zero because, after conditioning on the observed $r_1(1), \dots, r_N(1), r_1(0), \dots, r_N(0)$, the expected value of the first factor is $0$. \\

The first term simplifies to

\begin{align*}
\ex\left[  \left(  \overline{r_t} - \overline{r_c} - \left(\overline{r(1)} - \overline{r(0)}\right) \right)^2\right]  &= \ex\left[  \left(  (\overline{r_t} - \overline{r(1)} )- ( \overline{r_c} -  \overline{r(0)}) \right)^2\right] \\
&= \ex\left[   (\overline{r_t} - \overline{r(1)} )^2 \right] +  \ex\left[ ( \overline{r_c} -  \overline{r(0)} )^2\right]  - 2 \ex\left[ (\overline{r_t} - \overline{r(1)} ) ( \overline{r_c} -  \overline{r(0)}) )\right] \\
&= \frac{\var(r(1))}{N_t} + \frac{\var(r(0))}{N_c} - \frac{\var(r(1) - r(0))}{N}
\end{align*}

by finite sample results (\textcolor{red}{cite the imbens and rubin text}).  The second term is simply the variance of the difference in means of all potential outcomes.  Thus, by definition

$$ \ex\left[ \left( (\overline{r(1)} - \overline{r(0)}) -  \ex_{SP}\left[ r(1) - r(0) \right] \right)^2 \right] = \frac{\var(r(1) - r(0))}{N}$$

Combining the three terms gives

\begin{equation}\label{eqn:tau_hat_adj_unsimplified}
\var(\hat{\tau}^{adj}) =\frac{\var(r(1))}{N_t} + \frac{\var(r(0))}{N_c}
\end{equation}

In terms of known quantities:

\begin{align*}
\var(r(1)) &= \var(Y(1) - \hat{Y}) \\
&= \var(Y(1)) + \var(\hat{Y}) - 2\cov(Y(1), \hat{Y}) \\
&= \sigma_1^2 + \nu^2 - 2 \rho_1 \sigma_1 \nu
\end{align*}

where $\nu^2 := \var(\hat{Y})$ and $\rho_1$ is the correlation between $Y(1)$ and $\hat{Y}$.
This expression can be rearranged as
\begin{align*}
\var(r(1)) &= (\sigma_1 - \nu)^2 - 2(1-\rho_1)\sigma_1\nu
\end{align*}

Similarly,

\begin{equation*}
\var(r(0)) =  (\sigma_0 - \nu)^2 - 2(1-\rho_0)\sigma_0\nu
\end{equation*}

where $\rho_0$ is the correlation between $Y(0)$ and $\hat{Y}$. Plugging these expressions into equation~\ref{eqn:tau_hat_adj_unsimplified} gives the desired result.
\end{proof}


\bibliographystyle{plainnat}
\bibliography{refs}



\end{document}